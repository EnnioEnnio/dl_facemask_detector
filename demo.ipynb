{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7018e07-67bf-4281-9ff8-b457c668f0f7",
   "metadata": {},
   "source": [
    "# Facemask Classifier\n",
    "\n",
    "This notebook serves as a basic showcase of this project and the results we achieved.\n",
    "\n",
    "> **Note:** we assume that at this point you have already set up a virtual environment as outlined in the [README](./README.md) and that all necessary dependencies are installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd5bfd-5ab2-42b4-a35d-2a1dc80aa0c7",
   "metadata": {},
   "source": [
    "First things first, get imports and setup out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9163bf-a16a-4d83-98de-08e8672b6b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ed172-66f7-4c46-ab8f-067e439a573b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for the sake of consistency and reproducability in this demo, we set a\n",
    "# fixed seed. But feel free to play around with it or omit this part.\n",
    "seed=1337\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb4b9d4-965c-4864-80ef-95026bbc64e2",
   "metadata": {},
   "source": [
    "Let's download the evaluation dataset. Note, the model was **not** trained on these images, although they stem from the same dataset. Refer to the project's [README](./README.md) for the original dataset source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d259703-2c02-47e9-9abe-aa2023d8ff18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from util import download_dataset, dataset_url\n",
    "dataset_path='dataset'\n",
    "download_dataset(dataset_url, out_path=dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ddc56-d267-452c-a353-814631339166",
   "metadata": {},
   "source": [
    "Now for the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c3c6d-5bdb-46f4-a13e-4e42ea2b2e48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from util import download_model, model_url\n",
    "model_path='model.pt'\n",
    "download_model(model_url, out_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98372f-5e58-45ee-9837-0c9ff48095b4",
   "metadata": {},
   "source": [
    "Let's choose a small subsample of images from the dataset and run our model against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9ec661-9886-4f88-8574-3117a8a812cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from architecture import Model1\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "model = Model1()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "shared_transforms = transforms.Compose([transforms.ToTensor(), transforms.Resize((256, 256), antialias=True)])\n",
    "\n",
    "# collect data for displaying later\n",
    "eval_results=[]\n",
    "\n",
    "testset_path = os.path.join(dataset_path, 'test')\n",
    "all_test_imgs = ImageFolder(root=testset_path, transform=shared_transforms)\n",
    "\n",
    "# try to use a perfect square, makes plotting calculations nicer :)\n",
    "num_samples=25\n",
    "test_img_subset = Subset(all_test_imgs, random.sample(range(0, len(all_test_imgs)-1), num_samples))\n",
    "test_loader = DataLoader(test_img_subset, shuffle=True)\n",
    "\n",
    "def get_class(idx):\n",
    "    return all_test_imgs.classes[idx]\n",
    "\n",
    "num_correct = 0\n",
    "for data, label in test_loader:\n",
    "    actual = label.item()\n",
    "    prediction = 1 if torch.sigmoid(model(data)).item() > 0.5 else 0\n",
    "    print(\n",
    "        f\"True label: {actual} ({get_class(actual)}), Predicted label: {prediction} ({get_class(prediction)})\"\n",
    "    )\n",
    "    eval_results.append( (data.squeeze(), prediction, actual) )\n",
    "    if actual == prediction:\n",
    "        num_correct += 1\n",
    "print(f\"Accuracy: {num_correct/num_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c103a974-a4d7-4957-ba3a-56f258b31df6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt_samples=[test_img_subset.dataset.imgs[x] for x in test_img_subset.indices]\n",
    "axis = int(np.ceil(np.sqrt(num_samples)))\n",
    "fig, ax = plt.subplots(axis, axis, figsize = (16, 16))\n",
    "i = 0\n",
    "for row in range(axis):\n",
    "     ax_row = ax[row]\n",
    "     for column in range(axis):\n",
    "         ax_column = ax_row[column]\n",
    "         ax_column.set_xticklabels([])\n",
    "         ax_column.set_yticklabels([])\n",
    "         img = to_pil_image(eval_results[i][0])\n",
    "         ax_column.imshow(img, cmap='gray')\n",
    "         actual = eval_results[i][2]\n",
    "         predicted = eval_results[i][1]\n",
    "         is_correct = actual == predicted\n",
    "         col = 'blue'\n",
    "         ax_column.set_title(f\"actual: {get_class(actual)}\\npredicted: {get_class(predicted)}\",\n",
    "                    color = 'green' if is_correct else 'red')\n",
    "         i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f352a-d98e-4ca2-900b-c7164cbf75ce",
   "metadata": {},
   "source": [
    "Not too bad. Let's run another evaluation loop and collect some metrics, this time on the entirety of the testset (256 images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576583c1-2375-4244-9396-9ad0e865ddb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model_with_metrics(test_loader, predict_mask_label=1):\n",
    "    labels_true = []\n",
    "    labels_predictions = []\n",
    "    for data, label in test_loader:\n",
    "        actual = label.item()\n",
    "        labels_true.append(actual)\n",
    "        # NOTE: some datasets will make 'masked' have a label of 0.\n",
    "        # Since our model is trained to equate 1 == masked, we manually\n",
    "        # flip the label here if necessary.\n",
    "        prediction = predict_mask_label if torch.sigmoid(model(data)).item() > 0.5 else (1-predict_mask_label)\n",
    "        labels_predictions.append(prediction)\n",
    "\n",
    "    labels_true = np.array(labels_true)\n",
    "    labels_predictions = np.array(labels_predictions)\n",
    "\n",
    "    # calculate metrics\n",
    "    precision = precision_score(labels_true, labels_predictions)\n",
    "    recall = recall_score(labels_true, labels_predictions)\n",
    "    accuracy = accuracy_score(labels_true, labels_predictions)\n",
    "    f1 = f1_score(labels_true, labels_predictions)\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Model name: {model.__class__.__name__}\n",
    "    Total samples: {len(test_loader)}\n",
    "    Total correct: {np.sum(labels_true == labels_predictions)}\n",
    "    Accuracy: {accuracy}\n",
    "    Precision: {precision}\n",
    "    Recall: {recall}\n",
    "    F1 Score: {f1}\n",
    "    \"\"\")\n",
    "    \n",
    "    return labels_true, labels_predictions\n",
    "\n",
    "test_set = all_test_imgs # use all images instead of a subsample\n",
    "test_loader = DataLoader(test_set, shuffle=True)\n",
    "labels_true, labels_predictions = evaluate_model_with_metrics(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d8e1f-13b4-49aa-91ec-aa688f26bca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now let's visualize our metrics in a confusion matrix\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, title):\n",
    "    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    classes = ['Masked', 'Unmasked']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = conf_matrix.max() / 2.\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            plt.text(j, i, format(conf_matrix[i, j], 'd'),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# use the labels we just got from our evaluation run above\n",
    "conf_matrix = confusion_matrix(labels_true, labels_predictions)\n",
    "plot_confusion_matrix(conf_matrix, title='Model Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d2e40a-ac5c-48cb-a3aa-17f116d1e147",
   "metadata": {},
   "source": [
    "As you can see, the model performs decently on test images from the same dataset. But what about generalization? During training, we observed some overfitting. With this in mind, how well does our model generalize to other facemask datasets? Let's quickly compare.\n",
    "\n",
    "For this we will be using Kaggle user **pranavsingaraju**'s facemask dataset, which can be found [here](https://www.kaggle.com/datasets/pranavsingaraju/facemask-detection-dataset-20000-images). We have once again mirrored the dataset on Google Drive to make downloading more convenient, as Kaggle requires API tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c2f0cb-fc83-4cdf-8951-2fbfa46eaba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generalization_dataset_path='dataset-generalization'\n",
    "dataset_url='https://drive.google.com/uc?id=1zsCPUhyPL6ndkXdVJlF16JoBVpJ5meZT'\n",
    "download_dataset(dataset_url, out_path=generalization_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aeb9a8-b623-48ea-a442-0b27cbe59103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generalization_imgs = ImageFolder(root=os.path.abspath(generalization_dataset_path), transform=shared_transforms)\n",
    "generalization_loader = DataLoader(generalization_imgs, shuffle=True)\n",
    "\n",
    "labels_true, labels_predictions = evaluate_model_with_metrics(generalization_loader, predict_mask_label=0)\n",
    "conf_matrix = confusion_matrix(labels_true, labels_predictions)\n",
    "plot_confusion_matrix(conf_matrix, title='Model Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66a3478-8d59-4a9b-acac-bb8effc8164e",
   "metadata": {},
   "source": [
    "Still better than random guessing. The model seems to have a tendency to over-eagarly predict an individual to be wearing a mask, although false negatives remain quite low.\n",
    "\n",
    "This concludes the demo for our model. You can find more information, discussions, etc. in the project [report](./report/report.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
